# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19vxm01Qtv93tN048n1y8F7B-03Z69P1Y
"""

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input, Dropout
from tensorflow.keras import layers, models
from tensorflow.keras.models import Model
from tensorflow.keras.utils import img_to_array, load_img
import pandas as pd
import tensorflow as tf
import math
import glob
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import VGG16
import cv2
import numpy as np

print("Num GPUs Available:", len(tf.config.list_physical_devices('GPU')))
print("GPU Device:", tf.test.gpu_device_name())

def mask_generator(im):
  imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
  ret,thresh = cv2.threshold(imgray,127,255,0)
  contours, useless1 = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
  ellipse = cv2.fitEllipse(contours[0])
  return cv2.ellipse(im,ellipse,(255,255,255),-1)

train_dataset_paths = sorted(glob.glob("/content/drive/MyDrive/1327317/training_set/*.png"))
image_paths = [p for p in train_dataset_paths if "Annotation" not in p]
mask_paths = [ p for p in train_dataset_paths if "Annotation" in p]

print(len(mask_paths))

print(mask_paths[0])

import os
output_path = "/content/drive/MyDrive/1327317/training_set/mask"

for idx, path in enumerate(mask_paths):
    im = cv2.imread(path)
    mask = mask_generator(im)

    filename = f"{idx:03d}_mask.png"
    save_path = os.path.join(output_path, filename)

    cv2.imwrite(save_path, mask)

mask_paths = sorted(glob.glob("/content/drive/MyDrive/1327317/training_set/mask/*.png"))
print(len(mask_paths))

def load_dataset(images,masks):

  train_x, temp_x, train_y, temp_y = train_test_split(images, masks, test_size=0.2, random_state=42)
  val_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.5, random_state=42)
  return (train_x, test_x), (train_y, test_y), (val_x, val_y)

(train_x, test_x), (train_y, test_y), (val_x, val_y) = load_dataset(image_paths,mask_paths)
def show_image_mask(image_path, mask_path):
    image = cv2.imread(image_path)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 6))
    ax1.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    ax1.set_title("Image")
    ax1.axis("off")

    ax2.imshow(mask, cmap="gray")
    ax2.set_title("Mask")
    ax2.axis("off")

    plt.show()

print("Training dataset:")
show_image_mask(train_x[0], train_y[0])


print("test dataset:")
show_image_mask(test_x[0], test_y[0])

print("val dataset:")
show_image_mask(val_x[0], val_y[0])

print(len(train_x), len(val_x), len(test_x))

def read_image(path):
  path = path.decode()
  x = cv2.imread(path, cv2.IMREAD_COLOR)
  x = cv2.resize(x, (256, 256))
  x = x.astype(np.float32)
  x = x/255.0
  return x

def read_mask(path):
  path = path.decode()
  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
  x = cv2.resize(x, (256, 256))
  x = x.astype(np.float32)
  x = x/255.0
  x = np.expand_dims(x, axis=-1)
  return x

def tf_parse(x, y):
  def _parse(x, y):
    x = read_image(x)

    y = read_mask(y)
    return x,y

  x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])
  x.set_shape([256, 256, 3])
  y.set_shape([256, 256, 1])
  return x, y

def tf_dataset(X,Y,batch = 16):
  dataset = tf.data.Dataset.from_tensor_slices((X,Y))
  dataset = dataset.map(tf_parse)
  dataset = dataset.batch(batch)
  dataset = dataset.prefetch(10)
  return dataset

train_dataset = tf_dataset(train_x, train_y, batch=32)
val_dataset = tf_dataset(val_x, val_y, batch=32)
test_dataset = tf_dataset(test_x, test_y, batch=32)

for x,y in train_dataset:
  print(x.shape, y.shape)
  break

def conv_block(input, num_filters):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

def decoder_block(input, skip_features, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters)
    return x

def build_vgg16_unet(input_shape):
    """ Input """
    inputs = Input(input_shape)

    """ Pre-trained VGG16 Model """
    vgg16 = VGG16(include_top=False, weights="imagenet", input_tensor=inputs)

    """ Encoder """
    s1 = vgg16.get_layer("block1_conv2").output         ## (256 x 256)
    s2 = vgg16.get_layer("block2_conv2").output         ## (128x 128)
    s3 = vgg16.get_layer("block3_conv3").output         ## (64 x 64)
    s4 = vgg16.get_layer("block4_conv3").output         ## (32 x 32)

    """ Bridge """
    b1 = vgg16.get_layer("block5_conv3").output         ## (32 x 32)

    """ Decoder """
    d1 = decoder_block(b1, s4, 512)                     ## (32 x 32)
    d2 = decoder_block(d1, s3, 256)                     ## (64 x 64)
    d3 = decoder_block(d2, s2, 128)                     ## (128 x 128)
    d4 = decoder_block(d3, s1, 64)                      ## (256 x 256)

    """ Output """
    outputs = Conv2D(1, 1, padding="same", activation="sigmoid")(d4)

    model = Model(inputs, outputs, name="VGG16_U-Net_test")
    return model

model = build_vgg16_unet((256, 256, 3))
model.summary()

smooth = 1
def dice_coef(y_true, y_pred):
    y_true = tf.keras.layers.Flatten()(y_true)
    y_pred = tf.keras.layers.Flatten()(y_pred)
    intersection = tf.reduce_sum(y_true * y_pred)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)

def dice_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred)

model.compile(optimizer="adam", loss=dice_loss, metrics=[dice_coef])

model_path = "/content/drive/MyDrive/1327317/model.keras"

callback = [
    tf.keras.callbacks.ModelCheckpoint(model_path, verbose=1, save_best_only= True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)
]

history = model.fit(
    train_dataset,
    epochs= 50,
    validation_data= val_dataset,
    callbacks= callback
)

def plot_loss_and_dice(history):
    plt.figure(figsize=(12, 5))

    #Loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    #  Dice Coefficient
    plt.subplot(1, 2, 2)
    plt.plot(history.history['dice_coef'], label='Train Dice Coefficient')
    plt.plot(history.history['val_dice_coef'], label='Validation Dice Coefficient')
    plt.title('Dice Coefficient over Epochs')
    plt.xlabel('Epochs')
    plt.ylabel('Dice Coefficient')
    plt.legend()

    plt.show()

plot_loss_and_dice(history)

from tensorflow.keras.models import load_model
model = load_model('/content/drive/MyDrive/1327317/model.keras', custom_objects={
    'dice_loss': dice_loss,
    'dice_coef': dice_coef
})
model.summary()

print(test_x[0])

test_iterator = iter(test_dataset)
first_element = next(test_iterator)
first_input, first_label = first_element
print(first_input.shape, first_label.shape)
predict_mask_test = model.predict(first_input)

np.max(predict_mask_test[5])

print(np.unique(mask_test_5))

mask_test_5 = np.where(predict_mask_test[5] >= 0.5, 1, 0)

plt.imshow(mask_test_5, cmap='gray', interpolation='nearest')
plt.show()

print(type(first_label[0]))
first_label[5].numpy().squeeze()
plt.imshow(first_label[5], cmap='gray', interpolation='nearest')

type(first_input)
print(first_input.shape)
check = first_input[5].numpy().squeeze()
print(check.shape)
plt.imshow(first_input[5], interpolation='nearest')

def display_image(image_list):
  plt.figure(figsize =(15,15))
  title = ["Input Image", "Label", "Predicted Mask"]
  for i in range(len(image_list)):
    plt.subplot(1,len(image_list), i+1)
    plt.title(title[i])
    plt.imshow(image_list[i], cmap='gray', interpolation='nearest')
    plt.axis('off')
  plt.show()

def show_prediction(dataset = None, num=1):
  if dataset:
    for image, mask in dataset.take(num):
      pred_mask = model.predict(image)
      pred_mask_first = pred_mask[0]
      pred_mask_normalize = np.where(pred_mask_first >= 0.5 , 1, 0)
      image_array = image[0].numpy().squeeze()
      mask_array = mask[0].numpy().squeeze()
      display_image([image_array,mask_array, pred_mask_normalize])

show_prediction(test_dataset,3)

print(test_dataset)

all_predict = model.predict(test_dataset)

output_dir = "/content/drive/MyDrive/1327317/mask_test"
print(f"Output directory: {output_dir}")

import os
for i, filename in enumerate(test_x):
  mask = np.where(all_predict[i] >= 0.5 ,1,0).astype(np.uint8)
  mask_resized = cv2.resize(mask, (800, 540), interpolation=cv2.INTER_NEAREST)
  basename = os.path.basename(filename)
  output_filename = os.path.join(output_dir, f"{os.path.splitext(basename)[0]}.png")
  cv2.imwrite(output_filename, mask_resized * 255)
  print(f"Saved mask for {filename} to {output_filename}")

Y_test = np.concatenate([y.numpy() for x, y in test_dataset], axis=0).astype(np.float32)
print(Y_test.shape)

X_test = np.concatenate([x.numpy() for x, y in test_dataset], axis=0).astype(np.float32)
print(X_test.shape)
print(type(X_test))

y_pred_thresholded = (all_predict > 0.5).astype(np.float32)

dice_scores = [dice_coef(y_true, y_pred).numpy() for y_true, y_pred in zip(Y_test, y_pred_thresholded)]
mean_dice = np.mean(dice_scores)
print(f"Mean Dice Score on test set: {mean_dice:.4f}")

def fit_ellipse(im):
    imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)
    ret,thresh = cv2.threshold(imgray,127,255,0)
    contours, _ = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
    contour_sizes = [len(contour) for contour in contours]
    ellipse = cv2.fitEllipse(contours[np.argmax(contour_sizes)])
    return ellipse

df_train = pd.read_csv('/content/drive/MyDrive/1327317/training_set_pixel_size_and_HC.csv')
df_train.head()

HC_test_dataset = {'filename': [], 'semi_axes_a_mm': [], 'semi_axes_b_mm': [], 'HC_real_mm': []}

file_names = sorted([file for _, _, files in os.walk('/content/drive/MyDrive/1327317/mask_test') for file in files])

for file_name in file_names:
    im = cv2.imread(f'/content/drive/MyDrive/1327317/mask_test/{file_name}')
    ellipse = fit_ellipse(im)

    pixel_size = df_train.loc[df_train['filename'] == file_name, 'pixel size(mm)'].iloc[0]
    semi_axes_b, semi_axes_a = ellipse[1]
    hc_real_value = df_train.loc[df_train['filename'] == file_name, 'head circumference (mm)']

    if semi_axes_b > semi_axes_a:
        semi_axes_a, semi_axes_b = semi_axes_b, semi_axes_a

    HC_test_dataset['filename'].append(file_name)
    HC_test_dataset['semi_axes_a_mm'].append(semi_axes_a * pixel_size / 2)
    HC_test_dataset['semi_axes_b_mm'].append(semi_axes_b * pixel_size / 2)
    HC_test_dataset['HC_real_mm'].append(hc_real_value.values[0])

HC_test_dataset = pd.DataFrame(HC_test_dataset)
HC_test_dataset = HC_test_dataset[['filename', 'semi_axes_a_mm', 'semi_axes_b_mm', 'HC_real_mm']]

HC_test_dataset.to_csv("/content/drive/MyDrive/1327317/HC_test_dataset.csv")
print('creat HC_test_dataset successful!')

print(file_names)

print(HC_test_dataset)

def calculate_HC(a, b):
    return np.pi * (3 * (a + b) - np.sqrt((3 * a + b) * (a + 3 * b)))
HC_test_dataset["HC_mm"] = HC_test_dataset.apply(lambda row: calculate_HC(row["semi_axes_a_mm"], row["semi_axes_b_mm"]), axis=1)

output_path = "/content/drive/MyDrive/1327317/HC_test_dataset.csv"
HC_test_dataset.to_csv(output_path, index=False)

print("done")

HC_test_dataset.head()

mse = np.mean((HC_test_dataset['HC_real_mm'] - HC_test_dataset['HC_mm'])**2)
print(f"MSE: {mse:.4f}")
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")

mean_HC = HC_test_dataset['HC_real_mm'].mean()
print(f"Mean of Ground truth HC: {mean_HC:.4f} mm")

print(f"Percentage Of error between RMSE and Mean_HC: {rmse/mean_HC * 100: .2f}%")

test_dataset_paths = sorted(glob.glob("/content/drive/MyDrive/1327317/test_set/*.png"))
print(len(test_dataset_paths))
print(test_dataset_paths[0])

X_new = []
for img_path in test_dataset_paths :
    img = cv2.imread(img_path)
    img = cv2.resize(img, (256, 256))
    img = img / 255.0
    X_new.append(img)

X_new = np.array(X_new)

Y_new = model.predict(X_new)

print(X_new.shape)
print(Y_new.shape)

new_path = "/content/drive/MyDrive/1327317/new_data"
print(f"Output directory: {new_path}")

for i, filename in enumerate(test_dataset_paths):
  mask = np.where(Y_new[i] >= 0.5 ,1,0).astype(np.uint8)
  mask_resized = cv2.resize(mask, (800, 540), interpolation=cv2.INTER_NEAREST)
  basename = os.path.basename(filename)
  output_filename = os.path.join(new_path, f"{os.path.splitext(basename)[0]}.png")
  cv2.imwrite(output_filename, mask_resized * 255)
  print(f"Saved mask for {filename} to {output_filename}")

df_test = pd.read_csv('/content/drive/MyDrive/1327317/test_set_pixel_size.csv')
df_test.head()

import os
new_file_names = sorted([file for _, _, files in os.walk('/content/drive/MyDrive/1327317/new_data') for file in files])
print(new_file_names)

semi_axes_a_mm = []
semi_axes_b_mm = []
for filename in new_file_names:
    im = cv2.imread(f'/content/drive/MyDrive/1327317/new_data/{filename}')
    ellipse = fit_ellipse(im)
    semi_axes_b, semi_axes_a = ellipse[1]
    if semi_axes_b > semi_axes_a:
        semi_axes_a, semi_axes_b = semi_axes_b, semi_axes_a
    pixel_size = df_test.loc[df_test['filename'] == filename, 'pixel size(mm)'].iloc[0]
    semi_axes_a_mm.append(semi_axes_a * pixel_size / 2)
    semi_axes_b_mm.append(semi_axes_b * pixel_size / 2)

df_test['semi_axes_a_mm'] = semi_axes_a_mm
df_test['semi_axes_b_mm'] = semi_axes_b_mm
print("done")

df_test["HC_mm"] = df_test.apply(lambda row: calculate_HC(row["semi_axes_a_mm"], row["semi_axes_b_mm"]), axis=1)

output_path = "/content/drive/MyDrive/1327317/test_dataset_full.csv"
HC_test_dataset.to_csv(output_path, index=False)
print("done")

print(df_test)

